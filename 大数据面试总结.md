# 常用SQL

---

## SQL相关

分组操作会把分组的结果聚合成一条记录，而窗口函数是将分组的结果置于每一条数据记录中。

### 窗口函数语法

```sql
FUNCTION_NAME() OVER (
    PARTITION BY column1, column2, ...
    ORDER BY column3, column4, ...
    ROWS/RANGE BETWEEN ... AND ...
)
```

### 常见的窗口函数

#### 排序函数：

-   `ROW_NUMBER()`：为每一行分配一个唯一的序号（即使值相同，序号也不同）。 123
-   `RANK()`：为每一行分配一个排名（值相同时排名相同，后续排名会跳过）。 113
-   `DENSE_RANK()`：为每一行分配一个排名（值相同时排名相同，后续排名不跳过）。 112

#### 聚合函数：

-   `SUM()` 在窗口内计算聚合值
-   `AVG()`、`MIN()`、`MAX()`、`COUNT()`：在窗口内计算聚合值。

#### 跨行取值函数(偏移函数)：

-   `LEAD()`：获取当前行之后的某一行的值。
-   `LAG()`：获取当前行之前的某一行的值。
-   `FIRST_VALUE()`：获取窗口内某一列的第一个值
-   `LAST_VALUE()`: 获取窗口内某一列的最后一个值

#### 分布函数：

-   `NTILE(n)`：将数据分为 n 个桶，并为每一行分配一个桶编号。

### 3. 窗口函数的常见面试题

场景包括：排名、累计计算、滑动窗口、分组统计等

**连续登录天数：**

> 查询每个用户的连续登录天数。
> 使用 LAG() 和 SUM() 窗口函数实现。

**累计计算：**

> 查询每个用户的累计消费金额。
> 使用 SUM() OVER (PARTITION BY ... ORDER BY ...) 实现。

**排名问题：**

> 查询每个班级中成绩排名前 3 的学生。
> 使用 ROW_NUMBER() 或 RANK() 实现。

**滑动窗口计算：**

> 查询每个用户最近 7 天的平均消费金额。
> 使用 AVG() OVER (ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) 实现。

**分组统计：**

> 查询每个部门的工资排名。
> 使用 RANK() OVER (PARTITION BY department ORDER BY salary DESC) 实现。

---

## 开窗函数

聚合函数会把分组的结果聚合成一条记录，而窗口函数是将分组的结果置于每一条数据记录中。

`row_number() over( parttion by a order by b)`

### 排序：

-   `row_number()`：分组排序后，给每条数据生成一个序号。不重复，123
-   `rank()`:分组排序后，给每条数据生成一个序号。有重复，113
-   `dense_rank()`:分组排序后，给每条数据生成一个序号。有重复，112
-   `NTILE(n)`：将数据分为 n 个桶，并为每一行分配一个桶编号。

### 分组聚合:

-   `sum()`: 到当前行累加
-   `avg()`: 到当前行求平均值
-   `max()`: 到当前行最大值
-   `min()`: 到当前行最小值
-   `count()`: 到当前行条数

### 偏移

-   `lead()` : (领导) 看下一个。访问当前行之后的某一行的值。
-   `lag()` ：看上一个。

---

## 离线部分

### hive相关

#### hive

-   **内部表**: 完全管理表的生命周期。删除表时，元数据（表结构）和实际数据文件会被一起删除。
-   **外部表**: 仅管理元数据，不管理实际数据。删除表时，只删除元数据，实际数据文件保留在指定位置。

| 特性     | 内部表                 | 外部表                     |
| :------- | :--------------------- | :------------------------- |
| 数据管理 | Hive管理数据和元数据   | Hive仅管理元数据           |
| 删除表行为 | 删除元数据+数据        | 仅删除元数据               |
| 存储位置 | 默认仓库目录           | 用户指定路径（如HDFS）     |
| 共享性   | 不推荐共享             | 适合多工具共享             |
| 数据保留 | 临时数据               | 长期保留或外部依赖数据     |

### 数据倾斜

> 怎么解决数据倾斜的？
> 数据倾斜指的是在分布式计算中，某些节点处理的数据量远多于其他节点，导致这些节点成为瓶颈，拖慢整体任务执行速度。

### 拉链表

### 数据建模

-   数据分层
-   星形模型和雪花模型？

---

## 实时部分

### Kafka

> Kafka如何保证消息不丢失？

1.  **生产者发送消息到Brocker**
    -   同步发送:
    -   异步发送:
    -   记录发送失败的消息
    -   重试
2.  **消息在Broker中存储丢失**
3.  **消费者从Brocker中接收消息丢失**
    -   手动提交偏移量（Offset）
    -   消费幂等性

### Flink

> 1.Flink与Kafka数据完整性保障策略?

1.  **Kafka 数据完整性**
    -   生产者端：通过设置 acks=all 确保消息被所有副本确认后才视为发送成功，防止数据丢失。
    -   消费者端：启用自动提交偏移量（enable.auto.commit=true）或手动提交偏移量（enable.auto.commit=false），确保消息被正确处理后再提交偏移量。
2.  **Flink 数据完整性**
    -   Checkpointing：启用检查点机制，定期保存状态和偏移量，故障时从最近检查点恢复，避免数据丢失或重复处理。
    -   Exactly-Once 语义：通过检查点和两阶段提交（2PC）实现精确一次处理，确保数据不丢不重。
    -   状态后端：选择可靠的状态后端（如 RocksDB）持久化状态，防止故障时状态丢失。
3.  **Flink 与 Kafka 集成**
    -   Kafka Source：使用 FlinkKafkaConsumer 并启用检查点，确保偏移量随状态一起保存，故障时从检查点恢复。
    -   Kafka Sink：使用 FlinkKafkaProducer 并启用 Semantic.EXACTLY_ONCE，结合两阶段提交确保数据精确一次写入 Kafka。
4.  **容错与恢复**
    -   Task 故障恢复：Flink 自动重启失败任务并从检查点恢复，确保数据完整性。
    -   Job 故障恢复：手动或自动重启作业，从最近检查点恢复。
5.  **监控与告警**
    -   监控：实时监控 Flink 和 Kafka 的运行状态，及时发现并处理异常。
    -   告警：设置告警规则，在关键指标异常时通知运维人员。

---

## 面试题总结

### 常见linux命令:

-   `pwd` 查看当前目录
-   `ls -a` 列出目录下的所有文件（包括隐藏文件）
-   `find /path/to/search -name "filename"` 查找一个文件
-   `cat filename`
-   `less filename`
-   `tail -n 10 filename` # 查看最后 10 行 查看文件内容
-   `cp`
-   `scp`
-   `mv`
-   `rm -rf xxx.txt`
-   `df -h` 查看磁盘使用情况
-   `free -h` 查看内存使用情况

### 案例:

-   shell脚本定时删除日志文件
-   删除 7 天前的日志文件
-   shell脚本启动或者结束一个python程序

---

## 离线部分

---

## hadoop

> 有哪些组件？

---

## spark

1.  spark程序如何调试的？
2.  spark开窗用哪个方法？
3.  最擅长技能的是什么？

---

## 编程语言

### java基础

### python基础

-   python的GIL锁
-   asynio
-   多线程 IO密集型 网络请求，文件读写
-   多进程 CPU密集型

### scala基础

### Shell脚本中

-   `-eq`：等于 equal
-   `-ne`：不等于 not equal
-   `-lt`：小于 less than
-   `-le`：小于或等于 less than or equal to
-   `-gt`：大于 greater than
-   `-ge`：大于或等于 greater than or equal to

---

## 调度平台

### AirFlow

-   监听
-   工作流
-   任务
-   重试
-   告警

---

## oracle

-   函数
-   存储过程

---

## bi工具

-   帆软Bi
-   SmartBi

---

## python基础:

---

## java基础:

### java变量命名规范

-   不能数字开头
-   不能关键字
-   除非在循环或临时变量中，否则应避免使用单字符命名（如i、j）

### 数据类型

**基本数据类型**
-   整数 byte 1 short 2 int 4 long 8
-   浮点 float double
-   字符 char 'A'
-   布尔 boolean true false

**引用数据类型**
-   类
-   接口
-   数组
-   枚举

---

### 左++ 和 右++的区别

在 Java 中，`++` 是自增运算符，用于将变量的值增加 1

```java
int a=5;
int b=a++;

int c=5;
int d=++c;

// output
// a:6
// b:5
// c:6
// d:6
```

-   左++ 是先执行自增后赋值
-   右++ 是先赋值后执行自增

---

### try-catch-finally

```java
try{
	// 可能抛出异常的代码
}catch(Exception e){
	// 捕获并处理异常
}finally{
	// 无论是否发生异常，都会执行的代码
}
```

---

### for-each

```java
int[] arr={1,2,3,4,5};
for (int i :arr){
	System.out.println(i);
}
```

---

### 方法的重写和重载

-   重写是继承类重写方法。
-   方法重载是指在同一个类中定义多个方法，它们具有相同的名字但不同的参数列表。

---

### 多线程的实现方法

1.  继承Thread类,重写run()方法
2.  实现Runnable接口实现run()方法
3.  通过线程池管理多个线程，避免频繁创建和销毁线程的开销。

### static关键字的作用

### == 和 equals()

`==`

-   比较基本数据类型
-   比较值
-   比较引用类型(对象)
-   比较的是内存地址

`equals()`

-   比较引用类型(对象)
-   Object 类的默认实现与 `==` 相同，比较的是内存地址。
-   一些类重写了`equals()`方法，实现了比较对象内容。
-   也可以可重写`equals()`方法，以实现自定义比较逻辑。

### Java Bean 是一个特殊的 Java 类，通常用于封装数据。它遵循一些特定的约定，以便于适合与框架和工具集成。

### java集合

-   **List**
-   **Set**
-   **Queue**
-   **映射**
    -   Map

---

## 求职目标:

-   找个近点的
-   找个专业对口的
-   或者项目管理
-   事少，钱多，离家近。

### 岗位：大数据开发

1.  有大数据开发经验
2.  熟悉离线数据仓或实时数仓的设计、开发和维护
3.  掌握SparkSQL、Flink、Scala、Python、Java中的一种或多种技能
4.  对Hive表、flink流任务、spark Jar设计及开发和调优经验丰富者优先
5.  精通SQL，数据处理，数据分析。熟悉hive开发，熟练使用python的

---

## Flink CDC

### 一、基本概念

1.  Change Data Capture
2.
3.
4.

### 二、

```java
import com.ververica.cdc.connectors.mysql.MySQLSource;
import com.ververica.cdc.connectors.mysql.table.StartupOptions;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

public class MySQLCDCExample {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        MySQLSource<String> mySQLSource = MySQLSource.<String>builder()
            .hostname("localhost")
            .port(3306)
            .databaseList("test_db")
            .tableList("test_db.orders")
            .username("root")
            .password("password")
            .deserializer(new JsonDebeziumDeserializationSchema())
            .startupOptions(StartupOptions.initial())
            .build();

        env.addSource(mySQLSource)
            .print();

        env.execute("MySQL CDC Example");
    }
}
```

### Flink CDC 的启动模式有哪些？

-   `initial`：首次启动时读取全量数据，之后读取增量数据。
-   `earliest`：从最早的 binlog 位置开始读取。
-   `latest`：只读取最新的增量数据。
-   `timestamp`：从指定的时间戳开始读取。

---

> Flink CDC 是基于 Debezium 实现的，Debezium 是一个开源的 CDC 工具，负责解析数据库日志。
> Flink CDC 将 Debezium 的数据流集成到 Flink 的流处理引擎中。

---

> 寄件地址:广东省深圳市南山区豪方天际花园1栋26D 15967143744 王先生，
> 收件地址:广东省深圳市罗湖区莲塘D小区12栋2单元705 13267149013 汪先生

---

### hadoop的读写流程

### namenode

### ack 机制

---


